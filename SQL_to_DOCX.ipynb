{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Ce code python permet d'extraire le contenu pour une seule notion"
      ],
      "metadata": {
        "id": "n35nEtaldknT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SQL to DOCX\n",
        "\n",
        "import re\n",
        "import os\n",
        "import sqlparse\n",
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
        "\n",
        "# --- Configuration ---\n",
        "SQL_FILE_PATH = \"/content/TEXT.sql\"\n",
        "SINGLE_DOCX_OUTPUT_PATH = \"/content/philosophy_texts_by_notion_combination.docx\"\n",
        "\n",
        "SQL_COLUMN_INDICES_TO_EXTRACT = [1, 2, 3, 4, 5, 6, 7, 8] # Title, Intro, ..., PrimaryNotion, SecondaryNotion\n",
        "\n",
        "COL_IDX_TITLE = 0\n",
        "COL_IDX_INTRO = 1\n",
        "COL_IDX_BODY = 4\n",
        "COL_IDX_SOURCE = 5\n",
        "COL_IDX_NOTION_PRIMARY = 6      # SQL column 7\n",
        "COL_IDX_NOTION_SECONDARY = 7    # SQL column 8\n",
        "\n",
        "PHILOSOPHICAL_NOTIONS_LIST = sorted([ # Sorted for consistent processing if needed\n",
        "    \"art\", \"état\", \"conscience\", \"justice\", \"liberté\", \"nature\", \"raison\",\n",
        "    \"religion\", \"science\", \"technique\", \"vérité\", \"bonheur\", \"devoir\",\n",
        "    \"langage\", \"temps\", \"travail\", \"inconscient\"\n",
        "])\n",
        "# Ensure \"état\" is in the list, though it's handled specially\n",
        "if \"état\" not in PHILOSOPHICAL_NOTIONS_LIST:\n",
        "    PHILOSOPHICAL_NOTIONS_LIST.append(\"état\")\n",
        "    PHILOSOPHICAL_NOTIONS_LIST.sort()\n",
        "\n",
        "\n",
        "ETAT_SEUL_CATEGORY_TITLE = \"Textes convoquant seulement la notion “État”\"\n",
        "# --- End Configuration ---\n",
        "\n",
        "# Global counters\n",
        "insert_statements_found_g = 0\n",
        "rows_processed_count_g = 0\n",
        "texts_added_to_docx_g = 0 # Counts each time a text is appended to any category\n",
        "\n",
        "\n",
        "def repair_mojibake(text):\n",
        "    if not text: return text\n",
        "    try: return text.encode('latin-1').decode('utf-8')\n",
        "    except (UnicodeEncodeError, UnicodeDecodeError): return text\n",
        "\n",
        "def unescape_sql_string(s):\n",
        "    s = s.replace(\"''\", \"'\").replace('\"\"', '\"')\n",
        "    s = s.replace(\"\\\\\\\\\", \"\\\\\").replace(\"\\\\'\", \"'\").replace('\\\\\"', '\"')\n",
        "    s = s.replace(\"\\\\n\", \"\\n\").replace(\"\\\\r\", \"\\r\").replace(\"\\\\t\", \"\\t\")\n",
        "    return s\n",
        "\n",
        "def extract_sql_row_data(p_token, list_of_sql_indices_to_extract):\n",
        "    value_items_tokens = []\n",
        "    initial_sub_tokens = []\n",
        "    if hasattr(p_token, 'tokens'):\n",
        "        for sub_token in p_token.tokens:\n",
        "            if not sub_token.is_whitespace and sub_token.ttype != sqlparse.tokens.Punctuation:\n",
        "                initial_sub_tokens.append(sub_token)\n",
        "    if len(initial_sub_tokens) == 1:\n",
        "        content_string_token = initial_sub_tokens[0]\n",
        "        parsed_content_string = sqlparse.parse(str(content_string_token))\n",
        "        if parsed_content_string and len(parsed_content_string) > 0:\n",
        "            statement_from_reparse = parsed_content_string[0]\n",
        "            if statement_from_reparse.tokens and isinstance(statement_from_reparse.tokens[0], sqlparse.sql.IdentifierList):\n",
        "                value_items_tokens = list(statement_from_reparse.tokens[0].get_identifiers())\n",
        "            else:\n",
        "                for token_in_list in statement_from_reparse.tokens:\n",
        "                    if not token_in_list.is_whitespace and token_in_list.ttype != sqlparse.tokens.Punctuation:\n",
        "                        value_items_tokens.append(token_in_list)\n",
        "    else: value_items_tokens = initial_sub_tokens\n",
        "\n",
        "    max_needed_sql_index = max(list_of_sql_indices_to_extract) if list_of_sql_indices_to_extract else -1\n",
        "    if len(value_items_tokens) <= max_needed_sql_index: return None\n",
        "\n",
        "    extracted_data_row = []\n",
        "    for sql_idx in list_of_sql_indices_to_extract:\n",
        "        if sql_idx < len(value_items_tokens):\n",
        "            raw_value = str(value_items_tokens[sql_idx]).strip()\n",
        "            if (raw_value.startswith(\"'\") and raw_value.endswith(\"'\")) or \\\n",
        "               (raw_value.startswith('\"') and raw_value.endswith('\"')):\n",
        "                cleaned_value = unescape_sql_string(raw_value[1:-1])\n",
        "            elif raw_value.upper() == 'NULL': cleaned_value = ''\n",
        "            else: cleaned_value = raw_value\n",
        "            extracted_data_row.append(cleaned_value)\n",
        "        else: return None\n",
        "    return extracted_data_row if len(extracted_data_row) == len(list_of_sql_indices_to_extract) else None\n",
        "\n",
        "def capitalize_first_letter(text_string):\n",
        "    if not text_string: return \"\"\n",
        "    return text_string[0].upper() + text_string[1:]\n",
        "\n",
        "def capitalize_all_sentences(paragraph_text):\n",
        "    if not paragraph_text: return \"\"\n",
        "    paragraph_text = capitalize_first_letter(paragraph_text)\n",
        "    def cap_match(match): return match.group(1) + match.group(2).upper()\n",
        "    paragraph_text = re.sub(r'([.!?]\\s+)([a-z])', cap_match, paragraph_text)\n",
        "    return paragraph_text\n",
        "\n",
        "def parse_notions_from_cell(cell_content_str, valid_notions_list):\n",
        "    \"\"\"Parses a cell string and returns a set of valid notions found.\"\"\"\n",
        "    if not cell_content_str:\n",
        "        return set()\n",
        "\n",
        "    found_notions = set()\n",
        "    normalized_content = cell_content_str.lower().strip()\n",
        "\n",
        "    # Split by common delimiters like comma, semicolon, or multiple spaces.\n",
        "    potential_notions = re.split(r'[,;\\s]+', normalized_content)\n",
        "\n",
        "    for pn_candidate in potential_notions:\n",
        "        cleaned_pn = pn_candidate.strip()\n",
        "        if cleaned_pn in valid_notions_list: # Check against the master list\n",
        "            found_notions.add(cleaned_pn)\n",
        "    return found_notions\n",
        "\n",
        "# MODIFIED FUNCTION\n",
        "def add_document_heading(doc, text_content, level=2):\n",
        "    \"\"\"Adds a heading to the document.\n",
        "    The heading is formatted according to 'Heading {level}' style (e.g., 'Heading 2'),\n",
        "    then explicitly centered, set to Times New Roman font,\n",
        "    and followed by an empty paragraph (visual line break).\"\"\"\n",
        "\n",
        "    # Add the heading paragraph with the specified level\n",
        "    heading_paragraph = doc.add_heading(text_content, level=level)\n",
        "\n",
        "    # 1. Center the heading paragraph\n",
        "    heading_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "\n",
        "    # 3. Set the font to Times New Roman for all runs in the heading\n",
        "    #    add_heading usually creates a paragraph with one run for the text.\n",
        "    for run in heading_paragraph.runs:\n",
        "        run.font.name = 'Times New Roman'\n",
        "        # Note: Other font properties like size, bold, italic will be\n",
        "        # inherited from the 'Heading {level}' style unless also overridden here.\n",
        "        # If the 'Heading 2' style already defines font size, boldness, etc.,\n",
        "        # those will be retained, only the font family changes to Times New Roman.\n",
        "\n",
        "    # 2. Add a line break (an empty paragraph) after the heading\n",
        "    doc.add_paragraph()\n",
        "\n",
        "    return heading_paragraph\n",
        "# END OF MODIFIED FUNCTION\n",
        "\n",
        "def append_philosophy_text_to_doc(doc, title_text, intro_text, body_text_raw, source_text_raw, add_page_break_after):\n",
        "    default_font_name = 'Times New Roman'\n",
        "    default_font_size = Pt(12)\n",
        "\n",
        "    title_text_proc = capitalize_first_letter(str(title_text or \"\").strip())\n",
        "    intro_text_proc = capitalize_all_sentences(str(intro_text or \"\").strip())\n",
        "    body_text_raw = str(body_text_raw or \"\")\n",
        "    source_text_raw = str(source_text_raw or \"\")\n",
        "\n",
        "    p_title = doc.add_paragraph()\n",
        "    run_title = p_title.add_run(title_text_proc or \"No Title Provided\")\n",
        "    run_title.font.name = default_font_name; run_title.font.size = Pt(14); run_title.font.bold = True\n",
        "    doc.add_paragraph()\n",
        "\n",
        "    if intro_text_proc:\n",
        "        p_intro = doc.add_paragraph()\n",
        "        run_intro = p_intro.add_run(intro_text_proc)\n",
        "        run_intro.font.name = default_font_name; run_intro.font.size = default_font_size; run_intro.font.italic = True\n",
        "        p_intro.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
        "        doc.add_paragraph()\n",
        "    elif not (body_text_raw.strip() or source_text_raw.strip()): pass\n",
        "    else: doc.add_paragraph()\n",
        "\n",
        "    if body_text_raw.strip():\n",
        "        sql_paragraphs = body_text_raw.split('\\n')\n",
        "        tag_parser_regex = re.compile(r'<(b|i|u)>(.*?)</\\1>', re.IGNORECASE | re.DOTALL)\n",
        "        for sql_para_content in sql_paragraphs:\n",
        "            text_lines_from_br = re.split(r'<br\\s*/?>', sql_para_content, flags=re.IGNORECASE)\n",
        "            for line_content in text_lines_from_br:\n",
        "                p_body = doc.add_paragraph(); p_body.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
        "                current_pos = 0\n",
        "                while current_pos < len(line_content):\n",
        "                    match = tag_parser_regex.search(line_content, current_pos)\n",
        "                    if match:\n",
        "                        if line_content[current_pos:match.start()]:\n",
        "                            run = p_body.add_run(line_content[current_pos:match.start()])\n",
        "                            run.font.name = default_font_name; run.font.size = default_font_size\n",
        "                        tag_name, tag_content = match.group(1).lower(), match.group(2)\n",
        "                        run = p_body.add_run(tag_content)\n",
        "                        run.font.name = default_font_name; run.font.size = default_font_size\n",
        "                        if tag_name == 'b': run.font.bold = True\n",
        "                        elif tag_name == 'i': run.font.italic = True\n",
        "                        elif tag_name == 'u': run.font.underline = True\n",
        "                        current_pos = match.end()\n",
        "                    else:\n",
        "                        if line_content[current_pos:]:\n",
        "                            run = p_body.add_run(line_content[current_pos:])\n",
        "                            run.font.name = default_font_name; run.font.size = default_font_size\n",
        "                        current_pos = len(line_content)\n",
        "\n",
        "    if source_text_raw.strip():\n",
        "        doc.add_paragraph()\n",
        "        p_source = doc.add_paragraph(); p_source.alignment = WD_ALIGN_PARAGRAPH.RIGHT\n",
        "        source_tag_parser_regex = re.compile(r'<(b|i|u)>(.*?)</\\1>', re.IGNORECASE | re.DOTALL)\n",
        "        current_pos_source = 0\n",
        "        while current_pos_source < len(source_text_raw):\n",
        "            match_s = source_tag_parser_regex.search(source_text_raw, current_pos_source)\n",
        "            if match_s:\n",
        "                if source_text_raw[current_pos_source:match_s.start()]:\n",
        "                    run_s = p_source.add_run(source_text_raw[current_pos_source:match_s.start()])\n",
        "                    run_s.font.name = default_font_name; run_s.font.size = default_font_size\n",
        "                tag_name_s, tag_content_s = match_s.group(1).lower(), match_s.group(2)\n",
        "                run_s = p_source.add_run(tag_content_s)\n",
        "                run_s.font.name = default_font_name; run_s.font.size = default_font_size\n",
        "                if tag_name_s == 'b': run_s.font.bold = True\n",
        "                elif tag_name_s == 'i': run_s.font.italic = True\n",
        "                elif tag_name_s == 'u': run_s.font.underline = True\n",
        "                current_pos_source = match_s.end()\n",
        "            else:\n",
        "                if source_text_raw[current_pos_source:]:\n",
        "                    run_s = p_source.add_run(source_text_raw[current_pos_source:])\n",
        "                    run_s.font.name = default_font_name; run_s.font.size = default_font_size\n",
        "                current_pos_source = len(source_text_raw)\n",
        "\n",
        "    if add_page_break_after:\n",
        "        doc.add_page_break()\n",
        "\n",
        "def main():\n",
        "    global insert_statements_found_g, rows_processed_count_g, texts_added_to_docx_g\n",
        "\n",
        "    print(f\"Starting processing of SQL file: '{SQL_FILE_PATH}'\")\n",
        "    if not os.path.exists(SQL_FILE_PATH):\n",
        "        print(f\"ERROR: SQL input file not found: '{SQL_FILE_PATH}'\"); return\n",
        "\n",
        "    master_doc = Document()\n",
        "    # Set default font for 'Normal' style. Headings may have their own style definitions.\n",
        "    style = master_doc.styles['Normal']\n",
        "    style.font.name = 'Times New Roman'; style.font.size = Pt(12)\n",
        "\n",
        "    # It's good practice to define or check heading styles if specific looks are desired\n",
        "    # For example, to ensure 'Heading 2' has some base properties (though we override font and alignment below)\n",
        "    # heading2_style = master_doc.styles['Heading 2']\n",
        "    # heading2_style.font.size = Pt(13) # Example: set default size for Heading 2\n",
        "    # heading2_style.font.bold = True   # Example: set default bold for Heading 2\n",
        "\n",
        "\n",
        "    try:\n",
        "        with open(SQL_FILE_PATH, 'r', encoding='utf-8') as f: sql_content = f.read()\n",
        "    except Exception as e: print(f\"Error reading SQL file: {e}\"); return\n",
        "\n",
        "    parsed_statements = sqlparse.parse(sql_content)\n",
        "\n",
        "    categorized_texts = {} # Key: category title, Value: list of text_data dicts\n",
        "\n",
        "    print(\"\\nCategorizing texts based on notions 'état' and others...\")\n",
        "    for stmt in parsed_statements:\n",
        "        if stmt.get_type() != 'INSERT': continue\n",
        "        insert_statements_found_g += 1\n",
        "        row_tokens = []\n",
        "        values_token = next((t for t in stmt.tokens if isinstance(t, sqlparse.sql.Values)), None)\n",
        "        if values_token: row_tokens = [st for st in values_token.get_sublists() if isinstance(st, sqlparse.sql.Parenthesis)]\n",
        "        else:\n",
        "            after_values = False\n",
        "            for t in stmt.tokens:\n",
        "                if after_values and isinstance(t, sqlparse.sql.Parenthesis): row_tokens.append(t)\n",
        "                elif not after_values and t.is_keyword and t.normalized == 'VALUES': after_values = True\n",
        "                elif after_values and not t.is_whitespace and str(t) != ',': break\n",
        "\n",
        "        for p_token_row in row_tokens:\n",
        "            rows_processed_count_g += 1\n",
        "            extracted = extract_sql_row_data(p_token_row, SQL_COLUMN_INDICES_TO_EXTRACT)\n",
        "            if not (extracted and len(extracted) == len(SQL_COLUMN_INDICES_TO_EXTRACT)):\n",
        "                continue\n",
        "\n",
        "            text_data_payload = {\n",
        "                \"title\": repair_mojibake(extracted[COL_IDX_TITLE]),\n",
        "                \"intro\": repair_mojibake(extracted[COL_IDX_INTRO]),\n",
        "                \"body\": repair_mojibake(extracted[COL_IDX_BODY]),\n",
        "                \"source\": repair_mojibake(extracted[COL_IDX_SOURCE]),\n",
        "                \"_raw_primary_notion\": extracted[COL_IDX_NOTION_PRIMARY],\n",
        "                \"_raw_secondary_notion\": extracted[COL_IDX_NOTION_SECONDARY]\n",
        "            }\n",
        "\n",
        "            primary_notions_str = repair_mojibake(extracted[COL_IDX_NOTION_PRIMARY])\n",
        "            secondary_notions_str = repair_mojibake(extracted[COL_IDX_NOTION_SECONDARY])\n",
        "\n",
        "            set_primary = parse_notions_from_cell(primary_notions_str, PHILOSOPHICAL_NOTIONS_LIST)\n",
        "            set_secondary = parse_notions_from_cell(secondary_notions_str, PHILOSOPHICAL_NOTIONS_LIST)\n",
        "\n",
        "            has_etat_primary = \"bonheur\" in set_primary\n",
        "            has_etat_secondary = \"bonheur\" in set_secondary\n",
        "\n",
        "            if not has_etat_primary and not has_etat_secondary:\n",
        "                continue\n",
        "\n",
        "            all_other_valid_notions = (set_primary | set_secondary) - {\"état\"}\n",
        "            all_other_valid_notions = {n for n in all_other_valid_notions if n in PHILOSOPHICAL_NOTIONS_LIST}\n",
        "\n",
        "\n",
        "            if not all_other_valid_notions:\n",
        "                category_key = ETAT_SEUL_CATEGORY_TITLE\n",
        "                if category_key not in categorized_texts: categorized_texts[category_key] = []\n",
        "                categorized_texts[category_key].append(text_data_payload)\n",
        "            else:\n",
        "                for other_notion in all_other_valid_notions:\n",
        "                    if other_notion == \"état\": continue\n",
        "\n",
        "                    category_key = f\"Bonheur et {other_notion.capitalize()}\"\n",
        "                    if category_key not in categorized_texts: categorized_texts[category_key] = []\n",
        "                    categorized_texts[category_key].append(text_data_payload)\n",
        "\n",
        "    if not categorized_texts:\n",
        "        print(\"No texts related to 'état' found for DOCX generation.\")\n",
        "    else:\n",
        "        print(f\"\\nFound {len(categorized_texts)} categories of texts involving 'état'. Generating DOCX...\")\n",
        "        first_section_written_to_doc = False\n",
        "\n",
        "        # Handle \"État seul\" category first if it exists\n",
        "        if ETAT_SEUL_CATEGORY_TITLE in categorized_texts:\n",
        "            add_document_heading(master_doc, ETAT_SEUL_CATEGORY_TITLE, level=2)\n",
        "            first_section_written_to_doc = True\n",
        "            texts_in_cat = categorized_texts[ETAT_SEUL_CATEGORY_TITLE]\n",
        "            for i, data in enumerate(texts_in_cat):\n",
        "                is_last_in_cat = (i == len(texts_in_cat) - 1)\n",
        "                append_philosophy_text_to_doc(master_doc, data[\"title\"], data[\"intro\"], data[\"body\"], data[\"source\"], add_page_break_after=not is_last_in_cat)\n",
        "                texts_added_to_docx_g +=1\n",
        "            # Remove the processed category to avoid processing it again\n",
        "            del categorized_texts[ETAT_SEUL_CATEGORY_TITLE]\n",
        "\n",
        "        # Sort remaining categories (État et X) alphabetically for consistent order\n",
        "        other_combo_keys = sorted(categorized_texts.keys())\n",
        "\n",
        "        for category_key in other_combo_keys:\n",
        "            if first_section_written_to_doc:\n",
        "                master_doc.add_page_break()\n",
        "            else:\n",
        "                first_section_written_to_doc = True # This is the first section being written\n",
        "\n",
        "            add_document_heading(master_doc, category_key, level=2)\n",
        "            texts_in_cat = categorized_texts[category_key]\n",
        "            for i, data in enumerate(texts_in_cat):\n",
        "                is_last_in_cat = (i == len(texts_in_cat) - 1)\n",
        "                append_philosophy_text_to_doc(master_doc, data[\"title\"], data[\"intro\"], data[\"body\"], data[\"source\"], add_page_break_after=not is_last_in_cat)\n",
        "                texts_added_to_docx_g +=1\n",
        "\n",
        "        try:\n",
        "            master_doc.save(SINGLE_DOCX_OUTPUT_PATH)\n",
        "            print(f\"\\nSuccessfully created DOCX: '{SINGLE_DOCX_OUTPUT_PATH}'\")\n",
        "        except Exception as e: print(f\"ERROR saving DOCX: {e}\")\n",
        "\n",
        "    print(\"\\n--- Summary ---\")\n",
        "    print(f\"Total INSERT statements found: {insert_statements_found_g}\")\n",
        "    print(f\"Total data rows processed: {rows_processed_count_g}\")\n",
        "    print(f\"Total text append operations to DOCX: {texts_added_to_docx_g}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Check for dependencies\n",
        "    missing_libs = []\n",
        "    try:\n",
        "        import docx\n",
        "    except ImportError:\n",
        "        missing_libs.append(\"python-docx\")\n",
        "    try:\n",
        "        import sqlparse\n",
        "    except ImportError:\n",
        "        missing_libs.append(\"sqlparse\")\n",
        "\n",
        "    if missing_libs:\n",
        "        print(f\"ERROR: Missing required libraries: {', '.join(missing_libs)}.\")\n",
        "        print(f\"Please install them by running: pip install {' '.join(missing_libs)}\")\n",
        "        exit(1)\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMtaSaKapkLq",
        "outputId": "a5ca1128-4f9c-469f-832d-4c4e7543beea",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting processing of SQL file: '/content/TEXT.sql'\n",
            "\n",
            "Categorizing texts based on notions 'état' and others...\n",
            "\n",
            "Found 13 categories of texts involving 'état'. Generating DOCX...\n",
            "\n",
            "Successfully created DOCX: '/content/philosophy_texts_by_notion_combination.docx'\n",
            "\n",
            "--- Summary ---\n",
            "Total INSERT statements found: 138\n",
            "Total data rows processed: 1185\n",
            "Total text append operations to DOCX: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ancienne version visant à extraire les textes sans les trier"
      ],
      "metadata": {
        "id": "ok6ku6aqd_Oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SQL to DOCX sans tri\n",
        "\n",
        "import re\n",
        "import os\n",
        "import sqlparse\n",
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
        "\n",
        "# --- Configuration ---\n",
        "SQL_FILE_PATH = \"/content/TEXT.sql\"\n",
        "SINGLE_DOCX_OUTPUT_PATH = \"/content/all_philosophy_texts_sentence_caps.docx\"\n",
        "\n",
        "SQL_COLUMN_INDICES_TO_EXTRACT = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "\n",
        "COL_IDX_TITLE = 0\n",
        "COL_IDX_INTRO = 1\n",
        "COL_IDX_BODY = 4\n",
        "COL_IDX_SOURCE = 5\n",
        "COL_IDX_NOTION = 6\n",
        "COL_IDX_SECONDARY_NOTION = 7\n",
        "# --- End Configuration ---\n",
        "\n",
        "insert_statements_found_g = 0\n",
        "rows_processed_count_g = 0\n",
        "primary_texts_added_g = 0\n",
        "secondary_texts_added_g = 0\n",
        "rows_with_state_in_col8_g = 0\n",
        "\n",
        "\n",
        "def repair_mojibake(text):\n",
        "    if not text: return text\n",
        "    try: return text.encode('latin-1').decode('utf-8')\n",
        "    except (UnicodeEncodeError, UnicodeDecodeError): return text\n",
        "\n",
        "def unescape_sql_string(s):\n",
        "    s = s.replace(\"''\", \"'\").replace('\"\"', '\"')\n",
        "    s = s.replace(\"\\\\\\\\\", \"\\\\\").replace(\"\\\\'\", \"'\").replace('\\\\\"', '\"')\n",
        "    s = s.replace(\"\\\\n\", \"\\n\").replace(\"\\\\r\", \"\\r\").replace(\"\\\\t\", \"\\t\")\n",
        "    return s\n",
        "\n",
        "def extract_sql_row_data(p_token, list_of_sql_indices_to_extract):\n",
        "    value_items_tokens = []\n",
        "    initial_sub_tokens = []\n",
        "    if hasattr(p_token, 'tokens'):\n",
        "        for sub_token in p_token.tokens:\n",
        "            if not sub_token.is_whitespace and sub_token.ttype != sqlparse.tokens.Punctuation:\n",
        "                initial_sub_tokens.append(sub_token)\n",
        "    if len(initial_sub_tokens) == 1:\n",
        "        content_string_token = initial_sub_tokens[0]\n",
        "        parsed_content_string = sqlparse.parse(str(content_string_token))\n",
        "        if parsed_content_string and len(parsed_content_string) > 0:\n",
        "            statement_from_reparse = parsed_content_string[0]\n",
        "            if statement_from_reparse.tokens and isinstance(statement_from_reparse.tokens[0], sqlparse.sql.IdentifierList):\n",
        "                value_items_tokens = list(statement_from_reparse.tokens[0].get_identifiers())\n",
        "            else:\n",
        "                for token_in_list in statement_from_reparse.tokens:\n",
        "                    if not token_in_list.is_whitespace and token_in_list.ttype != sqlparse.tokens.Punctuation:\n",
        "                        value_items_tokens.append(token_in_list)\n",
        "    else: value_items_tokens = initial_sub_tokens\n",
        "\n",
        "    max_needed_sql_index = max(list_of_sql_indices_to_extract) if list_of_sql_indices_to_extract else -1\n",
        "    if len(value_items_tokens) <= max_needed_sql_index: return None\n",
        "\n",
        "    extracted_data_row = []\n",
        "    for sql_idx in list_of_sql_indices_to_extract:\n",
        "        if sql_idx < len(value_items_tokens):\n",
        "            raw_value = str(value_items_tokens[sql_idx]).strip()\n",
        "            if (raw_value.startswith(\"'\") and raw_value.endswith(\"'\")) or \\\n",
        "               (raw_value.startswith('\"') and raw_value.endswith('\"')):\n",
        "                cleaned_value = unescape_sql_string(raw_value[1:-1])\n",
        "            elif raw_value.upper() == 'NULL': cleaned_value = ''\n",
        "            else: cleaned_value = raw_value\n",
        "            extracted_data_row.append(cleaned_value)\n",
        "        else: return None\n",
        "    return extracted_data_row if len(extracted_data_row) == len(list_of_sql_indices_to_extract) else None\n",
        "\n",
        "def capitalize_first_letter(text_string):\n",
        "    \"\"\"Capitalizes the first letter of a string if it's not empty.\"\"\"\n",
        "    if not text_string:\n",
        "        return \"\"\n",
        "    return text_string[0].upper() + text_string[1:]\n",
        "\n",
        "def capitalize_all_sentences(paragraph_text):\n",
        "    \"\"\"Capitalizes the first letter of each sentence in a paragraph.\"\"\"\n",
        "    if not paragraph_text:\n",
        "        return \"\"\n",
        "\n",
        "    # Capitalize the very first letter of the paragraph\n",
        "    paragraph_text = capitalize_first_letter(paragraph_text)\n",
        "\n",
        "    # Use regex to find sentence endings followed by a lowercase letter\n",
        "    # and capitalize that letter.\n",
        "    # This matches: (punctuation)(whitespace)(lowercase letter)\n",
        "    # (.!?) - common sentence endings\n",
        "    # \\s+   - one or more whitespace characters\n",
        "    # ([a-z]) - a lowercase letter (this will be capitalized)\n",
        "    def cap_match(match):\n",
        "        return match.group(1) + match.group(2).upper()\n",
        "\n",
        "    paragraph_text = re.sub(r'([.!?]\\s+)([a-z])', cap_match, paragraph_text)\n",
        "    return paragraph_text\n",
        "\n",
        "def append_philosophy_text_to_doc(doc, title_text, intro_text, body_text_raw, source_text_raw, add_page_break_after_this_text):\n",
        "    default_font_name = 'Times New Roman'\n",
        "    default_font_size = Pt(12)\n",
        "\n",
        "    title_text = str(title_text or \"\")\n",
        "    intro_text = str(intro_text or \"\") # Keep raw intro for now\n",
        "    body_text_raw = str(body_text_raw or \"\")\n",
        "    source_text_raw = str(source_text_raw or \"\")\n",
        "\n",
        "    # Capitalize first letter of title\n",
        "    title_text_processed = capitalize_first_letter(title_text.strip())\n",
        "\n",
        "    # Capitalize all sentences in the intro paragraph\n",
        "    intro_text_processed = capitalize_all_sentences(intro_text.strip())\n",
        "\n",
        "\n",
        "    # 1. Headline (Title)\n",
        "    p_title = doc.add_paragraph()\n",
        "    run_title = p_title.add_run(title_text_processed or \"No Title Provided\")\n",
        "    run_title.font.name = default_font_name; run_title.font.size = Pt(14); run_title.font.bold = True\n",
        "    doc.add_paragraph()\n",
        "\n",
        "    # 2. Introductory Paragraph\n",
        "    if intro_text_processed: # Check if processed intro_text has content\n",
        "        p_intro = doc.add_paragraph()\n",
        "        run_intro = p_intro.add_run(intro_text_processed) # Use fully capitalized version\n",
        "        run_intro.font.name = default_font_name; run_intro.font.size = default_font_size; run_intro.font.italic = True\n",
        "        p_intro.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
        "        doc.add_paragraph()\n",
        "    elif not (body_text_raw.strip() or source_text_raw.strip()): pass\n",
        "    else: doc.add_paragraph()\n",
        "\n",
        "    # 3. Body Text\n",
        "    if body_text_raw.strip():\n",
        "        sql_paragraphs = body_text_raw.split('\\n')\n",
        "        tag_parser_regex = re.compile(r'<(b|i|u)>(.*?)</\\1>', re.IGNORECASE | re.DOTALL)\n",
        "        for sql_para_content in sql_paragraphs:\n",
        "            text_lines_from_br = re.split(r'<br\\s*/?>', sql_para_content, flags=re.IGNORECASE)\n",
        "            for line_content in text_lines_from_br:\n",
        "                p_body = doc.add_paragraph(); p_body.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
        "                current_pos = 0\n",
        "                while current_pos < len(line_content):\n",
        "                    match = tag_parser_regex.search(line_content, current_pos)\n",
        "                    if match:\n",
        "                        if line_content[current_pos:match.start()]:\n",
        "                            run = p_body.add_run(line_content[current_pos:match.start()])\n",
        "                            run.font.name = default_font_name; run.font.size = default_font_size\n",
        "                        tag_name, tag_content = match.group(1).lower(), match.group(2)\n",
        "                        run = p_body.add_run(tag_content)\n",
        "                        run.font.name = default_font_name; run.font.size = default_font_size\n",
        "                        if tag_name == 'b': run.font.bold = True\n",
        "                        elif tag_name == 'i': run.font.italic = True\n",
        "                        elif tag_name == 'u': run.font.underline = True\n",
        "                        current_pos = match.end()\n",
        "                    else:\n",
        "                        if line_content[current_pos:]:\n",
        "                            run = p_body.add_run(line_content[current_pos:])\n",
        "                            run.font.name = default_font_name; run.font.size = default_font_size\n",
        "                        current_pos = len(line_content)\n",
        "\n",
        "    # 4. Source Text\n",
        "    if source_text_raw.strip():\n",
        "        doc.add_paragraph()\n",
        "        p_source = doc.add_paragraph(); p_source.alignment = WD_ALIGN_PARAGRAPH.RIGHT\n",
        "        source_tag_parser_regex = re.compile(r'<(b|i|u)>(.*?)</\\1>', re.IGNORECASE | re.DOTALL)\n",
        "        current_pos_source = 0\n",
        "        while current_pos_source < len(source_text_raw):\n",
        "            match_s = source_tag_parser_regex.search(source_text_raw, current_pos_source)\n",
        "            if match_s:\n",
        "                if source_text_raw[current_pos_source:match_s.start()]:\n",
        "                    run_s = p_source.add_run(source_text_raw[current_pos_source:match_s.start()])\n",
        "                    run_s.font.name = default_font_name; run_s.font.size = default_font_size\n",
        "                tag_name_s, tag_content_s = match_s.group(1).lower(), match_s.group(2)\n",
        "                run_s = p_source.add_run(tag_content_s)\n",
        "                run_s.font.name = default_font_name; run_s.font.size = default_font_size\n",
        "                if tag_name_s == 'b': run_s.font.bold = True\n",
        "                elif tag_name_s == 'i': run_s.font.italic = True\n",
        "                elif tag_name_s == 'u': run_s.font.underline = True\n",
        "                current_pos_source = match_s.end()\n",
        "            else:\n",
        "                if source_text_raw[current_pos_source:]:\n",
        "                    run_s = p_source.add_run(source_text_raw[current_pos_source:])\n",
        "                    run_s.font.name = default_font_name; run_s.font.size = default_font_size\n",
        "                current_pos_source = len(source_text_raw)\n",
        "\n",
        "    if add_page_break_after_this_text:\n",
        "        doc.add_page_break()\n",
        "\n",
        "def main():\n",
        "    global insert_statements_found_g, rows_processed_count_g\n",
        "    global primary_texts_added_g, secondary_texts_added_g, rows_with_state_in_col8_g\n",
        "\n",
        "    print(f\"Starting processing of SQL file: '{SQL_FILE_PATH}'\")\n",
        "    if not os.path.exists(SQL_FILE_PATH):\n",
        "        print(f\"ERROR: SQL input file not found: '{SQL_FILE_PATH}'\"); return\n",
        "\n",
        "    master_doc = Document()\n",
        "    style = master_doc.styles['Normal']\n",
        "    style.font.name = 'Times New Roman'; style.font.size = Pt(12)\n",
        "\n",
        "    try:\n",
        "        with open(SQL_FILE_PATH, 'r', encoding='utf-8') as f: sql_content = f.read()\n",
        "    except Exception as e: print(f\"Error reading SQL file: {e}\"); return\n",
        "\n",
        "    parsed_statements = sqlparse.parse(sql_content)\n",
        "    primary_texts_data, secondary_texts_data = [], []\n",
        "\n",
        "    print(\"\\nScanning SQL rows for 'state' in primary (col 7) and secondary (col 8) notions...\")\n",
        "    for stmt in parsed_statements:\n",
        "        if stmt.get_type() != 'INSERT': continue\n",
        "        insert_statements_found_g += 1\n",
        "        row_tokens = []\n",
        "        values_token = next((t for t in stmt.tokens if isinstance(t, sqlparse.sql.Values)), None)\n",
        "        if values_token: row_tokens = [st for st in values_token.get_sublists() if isinstance(st, sqlparse.sql.Parenthesis)]\n",
        "        else:\n",
        "            after_values = False\n",
        "            for t in stmt.tokens:\n",
        "                if after_values and isinstance(t, sqlparse.sql.Parenthesis): row_tokens.append(t)\n",
        "                elif not after_values and t.is_keyword and t.normalized == 'VALUES': after_values = True\n",
        "                elif after_values and not t.is_whitespace and str(t) != ',': break\n",
        "\n",
        "        for p_token_row in row_tokens:\n",
        "            rows_processed_count_g += 1\n",
        "            extracted = extract_sql_row_data(p_token_row, SQL_COLUMN_INDICES_TO_EXTRACT)\n",
        "            if extracted and len(extracted) == len(SQL_COLUMN_INDICES_TO_EXTRACT):\n",
        "                text_data = {\n",
        "                    \"title\": repair_mojibake(extracted[COL_IDX_TITLE]),\n",
        "                    \"intro\": repair_mojibake(extracted[COL_IDX_INTRO]),\n",
        "                    \"body\": repair_mojibake(extracted[COL_IDX_BODY]),\n",
        "                    \"source\": repair_mojibake(extracted[COL_IDX_SOURCE])\n",
        "                }\n",
        "                primary_notion_text = repair_mojibake(extracted[COL_IDX_NOTION]).lower()\n",
        "                secondary_notion_text = repair_mojibake(extracted[COL_IDX_SECONDARY_NOTION]).lower()\n",
        "\n",
        "                state_in_primary = \"justice\" in primary_notion_text\n",
        "                state_in_secondary = \"justice\" in secondary_notion_text\n",
        "\n",
        "                if state_in_secondary:\n",
        "                    rows_with_state_in_col8_g +=1\n",
        "\n",
        "                if state_in_primary:\n",
        "                    primary_texts_data.append(text_data)\n",
        "                elif state_in_secondary:\n",
        "                    secondary_texts_data.append(text_data)\n",
        "\n",
        "    if rows_with_state_in_col8_g > 0:\n",
        "        print(f\"Found {rows_with_state_in_col8_g} occurrences of 'state' in secondary concepts (column 8).\")\n",
        "    else:\n",
        "        print(\"No occurrences of 'state' found in secondary concepts (column 8).\")\n",
        "\n",
        "    if primary_texts_data:\n",
        "        print(f\"\\nAppending {len(primary_texts_data)} primary texts to DOCX...\")\n",
        "        for i, data in enumerate(primary_texts_data):\n",
        "            is_last_primary = (i == len(primary_texts_data) - 1)\n",
        "            add_break = not (is_last_primary and not secondary_texts_data)\n",
        "            append_philosophy_text_to_doc(master_doc, data[\"title\"], data[\"intro\"], data[\"body\"], data[\"source\"], add_break)\n",
        "            primary_texts_added_g += 1\n",
        "\n",
        "    if secondary_texts_data:\n",
        "        print(f\"\\nAppending {len(secondary_texts_data)} secondary texts to DOCX...\")\n",
        "\n",
        "        if primary_texts_data or (not primary_texts_data and secondary_texts_data):\n",
        "            pass\n",
        "\n",
        "        p_heading = master_doc.add_paragraph()\n",
        "        run_heading = p_heading.add_run(\"Secondary Texts\")\n",
        "        run_heading.font.name = 'Times New Roman'; run_heading.font.size = Pt(16); run_heading.font.bold = True\n",
        "        p_heading.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "        master_doc.add_paragraph()\n",
        "\n",
        "        for i, data in enumerate(secondary_texts_data):\n",
        "            add_break = (i < len(secondary_texts_data) - 1)\n",
        "            append_philosophy_text_to_doc(master_doc, data[\"title\"], data[\"intro\"], data[\"body\"], data[\"source\"], add_break)\n",
        "            secondary_texts_added_g += 1\n",
        "\n",
        "    if primary_texts_added_g > 0 or secondary_texts_added_g > 0:\n",
        "        try:\n",
        "            master_doc.save(SINGLE_DOCX_OUTPUT_PATH)\n",
        "            print(f\"\\nSuccessfully created single DOCX: '{SINGLE_DOCX_OUTPUT_PATH}'\")\n",
        "        except Exception as e: print(f\"ERROR saving DOCX: {e}\")\n",
        "    else:\n",
        "        print(\"\\nNo texts met criteria for DOCX generation.\")\n",
        "\n",
        "    print(\"\\n--- Summary ---\")\n",
        "    print(f\"Total INSERT statements found: {insert_statements_found_g}\")\n",
        "    print(f\"Total data rows processed: {rows_processed_count_g}\")\n",
        "    print(f\"Rows with 'state' in column 8 (secondary notions): {rows_with_state_in_col8_g}\")\n",
        "    print(f\"Primary texts ('state' in col 7) added to DOCX: {primary_texts_added_g}\")\n",
        "    print(f\"Secondary texts ('state' ONLY in col 8) added to DOCX: {secondary_texts_added_g}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try: import docx, sqlparse\n",
        "    except ImportError as e:\n",
        "        print(f\"Missing library: {e.name}. Install with: pip install python-docx sqlparse\")\n",
        "        exit(1)\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K-KO8wWSyL8",
        "outputId": "9f3cb804-2b8e-4dff-e6c6-8119a378984c",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting processing of SQL file: '/content/TEXT.sql'\n",
            "\n",
            "Scanning SQL rows for 'state' in primary (col 7) and secondary (col 8) notions...\n",
            "Found 38 occurrences of 'state' in secondary concepts (column 8).\n",
            "\n",
            "Appending 68 primary texts to DOCX...\n",
            "\n",
            "Appending 35 secondary texts to DOCX...\n",
            "\n",
            "Successfully created single DOCX: '/content/all_philosophy_texts_sentence_caps.docx'\n",
            "\n",
            "--- Summary ---\n",
            "Total INSERT statements found: 138\n",
            "Total data rows processed: 1185\n",
            "Rows with 'state' in column 8 (secondary notions): 38\n",
            "Primary texts ('state' in col 7) added to DOCX: 68\n",
            "Secondary texts ('state' ONLY in col 8) added to DOCX: 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wubTzqUAOm9g",
        "outputId": "3ba0766b-ebc2-48ce-b669-dad7c86fd797",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting conversion of '/content/TEXTES(1).sql' to '/content/TEXT_columns_2-7.csv'.\n",
            "Extracting data from SQL columns (1-based): [2, 3, 4, 5, 6, 7, 8].\n",
            "CSV header will be: ['SQL_Column_2', 'SQL_Column_3', 'SQL_Column_4', 'SQL_Column_5', 'SQL_Column_6', 'SQL_Column_7', 'SQL_Column_8']\n",
            "\n",
            "--- DETAILED DEBUG for extract_and_write_value_row (call #1) ---\n",
            "  Parenthesis (p_token) string: '(1, 'la vertu et la dÃ©cadence dans la politique grecque', 'montesquieu met en lumière le contraste entre les politiques grecs d\\'autrefois, qui valorisaient la vertu comme soutien du gouvernement pop...'\n",
            "  Initial sub_tokens (after filtering out '(' and ')' and whitespace): 1\n",
            "  SUCCESS: Extracted 13 items from IdentifierList.\n",
            "  Final value items for extraction (13): ['1', \"'la vertu et la dÃ©cadence dans la politique grecque'\", \"'montesquieu met en lumière le contraste entre les politiques grecs d\\\\\", \"'Les vertus civiques ont disparu.'\", \"'Montesquieu'\", \"'\\t«\\xa0Les politiques grecs qui vivaient dans le gouvernement populaire n\", \"'Montesquieu (1689-1755), De l’Esprit des lois, 1748. Livre III, chapi\", \"'liberté'\", \"'devoir'\", \"'essentiel/accidentel'\", \"'[A] - Questions d\\\\'analyse\\\\r\\\\n\\\\r\\\\n1) Comment les politiques grecs du \", \"'voici une possible analyse du texte de montesquieu :\\\\n\\\\n- le texte op\", \"'V: L\\\\'évolution des politiques grecques reflète un déplacement de la \"]\n",
            "  CSV row to write (before decision): ['la vertu et la dÃ©cadence dans la politique grecque', \"montesquieu met en lumière le contraste entre les politiques grecs d\\\\'autrefois, qui valorisaient la vertu comme soutien du gouvernement populaire, et les politiques contemporains qui se concentrent davantage sur les aspects Ã©conomiques. selon lui, lorsque la vertu dÃ©cline, l\\\\'ambition et l\\\\'avarice prennent place dans les cÂ£urs, altÃ©rant les dÃ©sirs et les valeurs de la sociÃ©tÃ©. la rÃ©publique en devient alors une dÃ©pouille, soumise au pouvoir de quelques individus et Ã\\xa0 la licence de tous.\", 'Les vertus civiques ont disparu.', 'Montesquieu', \"\\t«\\xa0Les politiques grecs qui vivaient dans le gouvernement populaire ne reconnaissaient d\\\\'autre force qui pût le soutenir que celle de la vertu. Ceux d\\\\'aujourd\\\\'hui ne nous parlent que de manufactures, de commerce, de finances, de richesses, et de luxe même. Lorsque cette vertu cesse, l\\\\'ambition entre dans les cœurs qui peuvent la recevoir, et l\\\\'avarice entre dans tous. Les désirs changent d\\\\'objets\\xa0; ce qu\\\\'on aimait on ne l\\\\'aime plus\\xa0; on était libre avec les lois, on veut être libre contre elles\\xa0; chaque citoyen est comme un esclave échappé de la maison de son maître\\xa0; ce qui était maxime, on l\\\\'appelle rigueur\\xa0; ce qui était règle, on l\\\\'appelle gêne\\xa0; ce qui était attention, on l\\\\'appelle crainte. C\\\\'est la frugalité qui est l\\\\'avarice, et non pas le désir d\\\\'avoir. Autrefois le bien des particuliers faisait le trésor public\\xa0; mais pour lors le trésor public devient le patrimoine des particuliers. La république est une dépouille\\xa0; et sa force n\\\\'est plus que le pouvoir de quelques citoyens et la licence de tous.\\xa0»<br/><b>Montesquieu</b>\\\\n\", 'Montesquieu (1689-1755), De l’Esprit des lois, 1748. Livre III, chapitre III..', 'liberté']\n",
            "--- END DETAILED DEBUG call #1 ---\n",
            "\n",
            "\n",
            "--- Summary ---\n",
            "Total INSERT statements found: 138\n",
            "Total data rows (Parenthesis objects) processed: 1185\n",
            "Total rows written to CSV: 1185\n",
            "\n",
            "Successfully converted data to '/content/TEXT_columns_2-7.csv'.\n",
            "\n",
            "First 5 lines of the CSV output (including header):\n",
            "SQL_Column_2,SQL_Column_3,SQL_Column_4,SQL_Column_5,SQL_Column_6,SQL_Column_7,SQL_Column_8\n",
            "la vertu et la dÃ©cadence dans la politique grecque,\"montesquieu met en lumière le contraste entre les politiques grecs d\\'autrefois, qui valorisaient la vertu comme soutien du gouvernement populaire, et les politiques contemporains qui se concentrent davantage sur les aspects Ã©conomiques. selon lui, lorsque la vertu dÃ©cline, l\\'ambition et l\\'avarice prennent place dans les cÂ£urs, altÃ©rant les dÃ©sirs et les valeurs de la sociÃ©tÃ©. la rÃ©publique en devient alors une dÃ©pouille, soumise au pouvoir de quelques individus et Ã  la licence de tous.\",Les vertus civiques ont disparu.,Montesquieu,\"\t« Les politiques grecs qui vivaient dans le gouvernement populaire ne reconnaissaient d\\'autre force qui pût le soutenir que celle de la vertu. Ceux d\\'aujourd\\'hui ne nous parlent que de manufactures, de commerce, de finances, de richesses, et de luxe même. Lorsque cette vertu cesse, l\\'ambition entre dans les cœurs qui peuvent la recevoir, et l\\'avarice entre dans tous. Les désirs changent d\\'objets ; ce qu\\'on aimait on ne l\\'aime plus ; on était libre avec les lois, on veut être libre contre elles ; chaque citoyen est comme un esclave échappé de la maison de son maître ; ce qui était maxime, on l\\'appelle rigueur ; ce qui était règle, on l\\'appelle gêne ; ce qui était attention, on l\\'appelle crainte. C\\'est la frugalité qui est l\\'avarice, et non pas le désir d\\'avoir. Autrefois le bien des particuliers faisait le trésor public ; mais pour lors le trésor public devient le patrimoine des particuliers. La république est une dépouille ; et sa force n\\'est plus que le pouvoir de quelques citoyens et la licence de tous. »<br/><b>Montesquieu</b>\\n\",\"Montesquieu (1689-1755), De l’Esprit des lois, 1748. Livre III, chapitre III..\",liberté\n",
            "\"l\\'union harmonieuse, condition de la libertÃ©\",\"montesquieu souligne l\\'importance de l\\'union harmonieuse dans un Ã©tat libre. selon lui, la tranquillitÃ© apparente dans une rÃ©publique peut Ãªtre le signe de l\\'absence de libertÃ©. il dÃ©fend l\",Demander la courage militaire et la timidité civique dans une république est impossible.,Montesquieu,\"\t« Demander, dans un État libre, des gens hardis dans la guerre et timides dans la paix, c\\'est vouloir des choses impossibles, et, pour règle générale, toutes les fois qu\\'on verra tout le monde tranquille dans un État qui se donne le nom de république, on peut être assuré que la liberté n\\'y est pas. Ce qu\\'on appelle union dans un corps politique est une chose très équivoque : la vraie est une union d\\'harmonie, qui fait que toutes les parties, quelque opposées qu\\'elles nous paraissent, concourent au bien général de la Société, comme des dissonances dans la musique concourent à l\\'accord total. Il peut y avoir de l\\'union dans un État où l\\'on ne croit voir que du trouble, c\\'est-à-dire une harmonie d\\'où résulte le bonheur, qui seul est la vraie paix. Il en est comme des parties de cet Univers, éternellement liées par l\\'action des unes et la réaction des autres. Mais, dans l\\'accord du despotisme asiatique, c\\'est-à-dire de tout gouvernement qui n\\'est pas modéré, il y a toujours une division réelle : le laboureur, l\\'homme de guerre, le négociant, le magistrat, le noble, ne sont joints que parce que les uns oppriment les autres sans résistance, et, si l\\'on y voit de l\\'union, ce ne sont pas des citoyens qui sont unis, mais des corps morts, ensevelis les uns auprès des autres. »<br/><b>Montesquieu</b>\\n\",,liberté\n",
            "la justice et la perspective de l\\'autre,\"leibniz aborde ici le principe de se mettre Ã  la place d\\'autrui pour juger de ce qui est juste ou non. il rÃ©pond Ã  certaines objections en expliquant que se mettre Ã  la place de l\\'autre ne signifie pas seulement prendre en compte ses intÃ©rÃªts individuels, mais aussi ceux des autres parties impliquÃ©es. il souligne ainsi l\\'importance de la justice distributive et de l\\'Ã©quitÃ© dans les relations sociales.\",Mettez-vous à la place d\\'autrui pour juger ce qui est juste ou non.,Leibniz,\"\t« Mettez-vous à la place d\\'autrui, et vous serez dans le vrai point de vue pour juger ce qui est juste ou non. On a fait quelques objections contre cette grande règle, mais elles viennent de ce qu\\'on ne l\\'applique point partout. On objecte par exemple qu\\'un criminel peut prétendre, en vertu de cette maxime, d\\'être pardonné par le juge souverain, parce que le juge souhaiterait la même chose, s\\'il était en pareille posture. La réponse est aisée. Il faut que le juge ne se mette pas seulement dans la place du criminel, mais encore dans celle des autres qui sont intéressés que le crime soit puni […]. Il en est de même de cette objection que la justice distributive demande une inégalité entre les hommes, que dans une société on doit partager le gain à proportion de ce que chacun a conféré&#x202F;<sup>(1)</sup> et qu\\'on doit avoir égard au mérite et au démérite. La réponse est encore aisée. Mettez-vous à la place de tous et supposez qu\\'ils soient bien informés et bien éclairés. Vous recueillerez de leurs suffrages cette conclusion qu\\'ils jugent convenable à leur intérêt qu\\'on distingue les uns des autres. Par exemple, si dans une société de commerce le gain n\\'était point partagé à proportion, l\\'on y entrerait point ou l\\'on en sortirait bientôt, ce qui est contre l\\'intérêt de toute la société. »<br/><b>Leibniz</b>\",,justice\n",
            "les pensÃ©es fanatiques et leur emportement,\"alain souligne le phÃ©nomÃ¨ne des fanatiques, qui, bien qu\\'honorables Ã  leurs propres yeux, commettent des crimes par passion pour leurs idÃ©es. il met en avant le manque de pensÃ©e critique et d\\'ouverture d\\'esprit de ces individus, qui se limitent Ã  une seule perspective, ce qui les empÃªche de persuader par la rÃ©flexion libre. les pensÃ©es fanatiques sont comparÃ©es Ã  des passions dÃ©chaÃ®nÃ©es, dÃ©pourvues de doute, qui ne se gouvernent pas elles-mÃªmes.\",\"On a toujours admiré les fanatiques, mais c\\'est leur pensée qui est à vérifier.\",Alain,\"\t« On a vu des fanatiques en tous les temps, et sans doute honorables à leurs propres yeux. Ces crimes&#x202F;<sup>(1)</sup> sont la suite d\\'une idée, religion, justice, liberté. Il y a un fond d\\'estime, et même quelquefois une secrète admiration, pour des hommes qui mettent au jeu leur propre vie, et sans espérer aucun avantage ; car nous ne sommes points fiers de faire si peu et de risquer si peu pour ce que nous croyons juste ou vrai. Certes je découvre ici des vertus rares, qui veulent respect, et une partie au moins de la volonté. Mais c\\'est à la pensée qu\\'il faut regarder. Cette pensée raidie, qui se limite, qui ne voit qu\\'un côté, qui ne comprend point la pensée des autres, ce n\\'est point la pensée Il y a quelque chose de mécanique dans une pensée fanatique, car elle revient toujours par les mêmes chemins. Elle ne cherche plus, elle n\\'invente plus. Le dogmatisme est comme un délire récitant. Il y manque cette pointe de diamant, le doute, qui creuse toujours. Ces pensées fanatiques gouvernent admirablement les peurs et les désirs, mais elles ne se gouvernent pas elles-mêmes. Elles ne cherchent pas ces vues de plusieurs points, ces perspectives sur l\\'adversaire, enfin cette libre réflexion qui ouvre les chemins de persuader, et qui détourne en même temps de forcer. Bref il y a un emportement de pensée, et une passion de penser qui ressemble aux autres passions. »<br/><b>Alain</b>\",,religion\n"
          ]
        }
      ],
      "source": [
        "#@title SQL to CSV\n",
        "\n",
        "import sqlparse\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "sql_file_path = \"/content/TEXTES(1).sql\"\n",
        "# Making the CSV filename more descriptive for multiple columns\n",
        "csv_file_path = \"/content/TEXT_columns_2-7.csv\"\n",
        "\n",
        "# SQL column indices to extract (0-based).\n",
        "# User wants SQL columns 2, 3, 4, 5, 6, 7.\n",
        "# In 0-based indexing, these are: 1, 2, 3, 4, 5, 6.\n",
        "sql_column_indices_to_extract = [1, 2, 3, 4, 5, 6, 7]\n",
        "\n",
        "# CSV header row, reflecting the original SQL column numbers\n",
        "csv_header = [f\"SQL_Column_{idx + 1}\" for idx in sql_column_indices_to_extract]\n",
        "# --- End Configuration ---\n",
        "\n",
        "print(f\"Starting conversion of '{sql_file_path}' to '{csv_file_path}'.\")\n",
        "print(f\"Extracting data from SQL columns (1-based): {[idx + 1 for idx in sql_column_indices_to_extract]}.\")\n",
        "print(f\"CSV header will be: {csv_header}\")\n",
        "\n",
        "\n",
        "if not os.path.exists(sql_file_path):\n",
        "    print(f\"ERROR: SQL input file not found at '{sql_file_path}'\")\n",
        "    exit()\n",
        "\n",
        "insert_statements_found = 0\n",
        "rows_processed_count = 0\n",
        "rows_written_to_csv = 0\n",
        "_debug_extract_counter = 0 # Counter for debugging specific calls if needed\n",
        "\n",
        "def extract_and_write_value_row(p_token, list_of_sql_indices, csv_w):\n",
        "    \"\"\"\n",
        "    Processes an sqlparse.sql.Parenthesis token (representing a row of values),\n",
        "    extracts the specified columns, cleans them, and writes them as a single row to the CSV.\n",
        "    p_token is expected to be an sqlparse.sql.Parenthesis object.\n",
        "    list_of_sql_indices are the 0-based indices to extract from the SQL data.\n",
        "    Returns a tuple: (bool_was_row_format_ok, bool_written_to_csv_this_row)\n",
        "    \"\"\"\n",
        "    global _debug_extract_counter, rows_written_to_csv # Allow modification of global rows_written_to_csv\n",
        "    _debug_extract_counter += 1\n",
        "\n",
        "    value_items = []\n",
        "\n",
        "    initial_sub_tokens = []\n",
        "    if hasattr(p_token, 'tokens'):\n",
        "        for sub_token in p_token.tokens:\n",
        "            if not sub_token.is_whitespace and sub_token.ttype != sqlparse.tokens.Punctuation:\n",
        "                initial_sub_tokens.append(sub_token)\n",
        "\n",
        "    # This detailed debug block can be re-enabled if issues arise with specific rows\n",
        "    # For now, it's condensed as the previous step confirmed this mechanism.\n",
        "    first_few_rows_debug = (_debug_extract_counter <= 3 and rows_written_to_csv == 0) # More targeted debug\n",
        "\n",
        "    if first_few_rows_debug:\n",
        "        print(f\"\\n--- DETAILED DEBUG for extract_and_write_value_row (call #{_debug_extract_counter}) ---\")\n",
        "        print(f\"  Parenthesis (p_token) string: '{str(p_token)[:200]}...'\")\n",
        "        print(f\"  Initial sub_tokens (after filtering out '(' and ')' and whitespace): {len(initial_sub_tokens)}\")\n",
        "\n",
        "    if len(initial_sub_tokens) == 1:\n",
        "        content_string_token = initial_sub_tokens[0]\n",
        "        parsed_content_string = sqlparse.parse(str(content_string_token))\n",
        "\n",
        "        if parsed_content_string and len(parsed_content_string) > 0:\n",
        "            statement_from_reparse = parsed_content_string[0]\n",
        "            if statement_from_reparse.tokens and isinstance(statement_from_reparse.tokens[0], sqlparse.sql.IdentifierList):\n",
        "                identifier_list_token = statement_from_reparse.tokens[0]\n",
        "                value_items = list(identifier_list_token.get_identifiers())\n",
        "                if first_few_rows_debug:\n",
        "                    print(f\"  SUCCESS: Extracted {len(value_items)} items from IdentifierList.\")\n",
        "            else: # Fallback if not IdentifierList (less likely for comma-separated values)\n",
        "                if first_few_rows_debug:\n",
        "                    print(\"  INFO: Re-parsed content's first token NOT IdentifierList. Iterating its tokens.\")\n",
        "                for token_in_list in statement_from_reparse.tokens:\n",
        "                    if not token_in_list.is_whitespace and token_in_list.ttype != sqlparse.tokens.Punctuation:\n",
        "                        value_items.append(token_in_list)\n",
        "        elif first_few_rows_debug:\n",
        "            print(f\"  WARNING: Re-parsing content string '{str(content_string_token)[:100]}...' yielded no statements.\")\n",
        "    elif first_few_rows_debug:\n",
        "         print(f\"  WARNING: Expected 1 central content token from Parenthesis, found {len(initial_sub_tokens)}. Using these directly (likely wrong).\")\n",
        "         value_items = initial_sub_tokens\n",
        "\n",
        "\n",
        "    max_needed_sql_index = -1\n",
        "    if list_of_sql_indices:\n",
        "        max_needed_sql_index = max(list_of_sql_indices)\n",
        "\n",
        "    if len(value_items) <= max_needed_sql_index:\n",
        "        if first_few_rows_debug:\n",
        "            print(f\"  FINAL WARNING: Need up to SQL index {max_needed_sql_index}, but only {len(value_items)} value items identified. Row skipped.\")\n",
        "            print(f\"    Identified items: {[str(item)[:30] for item in value_items]}...\")\n",
        "        if first_few_rows_debug: print(f\"--- END DETAILED DEBUG call #{_debug_extract_counter} ---\\n\")\n",
        "        return True, False\n",
        "\n",
        "    csv_row_to_write = []\n",
        "    all_cols_extracted_successfully = True\n",
        "    for sql_idx_to_extract in list_of_sql_indices:\n",
        "        # This check should be redundant given the max_needed_sql_index check above, but for safety:\n",
        "        if sql_idx_to_extract < len(value_items):\n",
        "            target_token = value_items[sql_idx_to_extract]\n",
        "            raw_value = str(target_token).strip()\n",
        "\n",
        "            if (raw_value.startswith(\"'\") and raw_value.endswith(\"'\")) or \\\n",
        "               (raw_value.startswith('\"') and raw_value.endswith('\"')):\n",
        "                cleaned_value = raw_value[1:-1]\n",
        "                if raw_value.startswith(\"'\"):\n",
        "                    cleaned_value = cleaned_value.replace(\"''\", \"'\")\n",
        "                else:\n",
        "                    cleaned_value = cleaned_value.replace('\"\"', '\"')\n",
        "            elif raw_value.upper() == 'NULL':\n",
        "                cleaned_value = ''\n",
        "            else:\n",
        "                cleaned_value = raw_value\n",
        "            csv_row_to_write.append(cleaned_value)\n",
        "        else:\n",
        "            # Should not happen if logic above is correct\n",
        "            if first_few_rows_debug:\n",
        "                 print(f\"  CRITICAL ERROR: Attempted to access SQL index {sql_idx_to_extract} \"\n",
        "                       f\"but only {len(value_items)} items. This indicates a logic flaw.\")\n",
        "            all_cols_extracted_successfully = False\n",
        "            break\n",
        "\n",
        "    if first_few_rows_debug:\n",
        "        print(f\"  Final value items for extraction ({len(value_items)}): {[str(item)[:70] for item in value_items]}\")\n",
        "        print(f\"  CSV row to write (before decision): {csv_row_to_write}\")\n",
        "        print(f\"--- END DETAILED DEBUG call #{_debug_extract_counter} ---\\n\")\n",
        "\n",
        "\n",
        "    if all_cols_extracted_successfully and len(csv_row_to_write) == len(list_of_sql_indices):\n",
        "        csv_w.writerow(csv_row_to_write)\n",
        "        return True, True\n",
        "\n",
        "    return True, False\n",
        "\n",
        "\n",
        "try:\n",
        "    with open(sql_file_path, 'r', encoding='utf-8') as sql_file, \\\n",
        "         open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_outfile:\n",
        "\n",
        "        csv_writer = csv.writer(csv_outfile)\n",
        "\n",
        "        # Write the header row to the CSV\n",
        "        if csv_header:\n",
        "            csv_writer.writerow(csv_header)\n",
        "\n",
        "        try:\n",
        "            sql_content = sql_file.read()\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading SQL file content: {e}\")\n",
        "            raise\n",
        "\n",
        "        parsed_statements = sqlparse.parse(sql_content)\n",
        "        # first_insert_structure_printed = False # Can be removed or kept for one-time structure check\n",
        "\n",
        "        for stmt_idx, stmt in enumerate(parsed_statements):\n",
        "            if stmt.get_type() != 'INSERT':\n",
        "                continue\n",
        "\n",
        "            insert_statements_found += 1\n",
        "\n",
        "            # Debug print for the structure of the first INSERT statement (can be commented out)\n",
        "            # if not first_insert_structure_printed and insert_statements_found == 1:\n",
        "            #     print(f\"\\n--- Structure of the FIRST INSERT statement found (Statement #{stmt_idx}) ---\")\n",
        "            #     # ... (full token printing code for the first INSERT statement)\n",
        "            #     print(f\"--- End structure of first INSERT statement ---\\n\")\n",
        "            #     first_insert_structure_printed = True\n",
        "\n",
        "            processed_values_for_this_stmt = False\n",
        "            for token in stmt.tokens:\n",
        "                if isinstance(token, sqlparse.sql.Values):\n",
        "                    sublists = token.get_sublists()\n",
        "                    for value_row_candidate in sublists:\n",
        "                        if isinstance(value_row_candidate, sqlparse.sql.Parenthesis):\n",
        "                            rows_processed_count += 1\n",
        "                            # Pass the list of indices to the modified function\n",
        "                            _, written = extract_and_write_value_row(\n",
        "                                value_row_candidate, sql_column_indices_to_extract, csv_writer\n",
        "                            )\n",
        "                            if written:\n",
        "                                rows_written_to_csv += 1\n",
        "                    processed_values_for_this_stmt = True\n",
        "                    break\n",
        "\n",
        "            if not processed_values_for_this_stmt:\n",
        "                is_after_values_keyword = False\n",
        "                for token_fb in stmt.tokens:\n",
        "                    if is_after_values_keyword:\n",
        "                        if isinstance(token_fb, sqlparse.sql.Parenthesis):\n",
        "                            rows_processed_count += 1\n",
        "                            _, written = extract_and_write_value_row(\n",
        "                                token_fb, sql_column_indices_to_extract, csv_writer\n",
        "                            )\n",
        "                            if written:\n",
        "                                rows_written_to_csv += 1\n",
        "                            processed_values_for_this_stmt = True\n",
        "                            break\n",
        "                        elif not token_fb.is_whitespace and not token_fb.is_comment:\n",
        "                            break\n",
        "                    if token_fb.is_keyword and token_fb.normalized == 'VALUES':\n",
        "                        is_after_values_keyword = True\n",
        "\n",
        "    print(\"\\n--- Summary ---\")\n",
        "    print(f\"Total INSERT statements found: {insert_statements_found}\")\n",
        "    print(f\"Total data rows (Parenthesis objects) processed: {rows_processed_count}\")\n",
        "    print(f\"Total rows written to CSV: {rows_written_to_csv}\")\n",
        "\n",
        "    if os.path.exists(csv_file_path) and rows_written_to_csv > 0:\n",
        "        print(f\"\\nSuccessfully converted data to '{csv_file_path}'.\")\n",
        "        with open(csv_file_path, 'r', encoding='utf-8') as f_verify:\n",
        "            print(\"\\nFirst 5 lines of the CSV output (including header):\")\n",
        "            for i, line in enumerate(f_verify):\n",
        "                if i >= 5: break\n",
        "                print(line.strip())\n",
        "    elif rows_processed_count > 0 and rows_written_to_csv == 0:\n",
        "         print(f\"\\nWarning: Processed {rows_processed_count} data rows but wrote 0 rows to CSV.\")\n",
        "         max_sql_col_needed = max(sql_column_indices_to_extract) + 1 if sql_column_indices_to_extract else 0\n",
        "         print(f\"This means that for all processed rows, the script did not find enough columns (at least {max_sql_col_needed} for the highest requested SQL column index) to extract all target columns.\")\n",
        "         print(\"If 'DETAILED DEBUG' messages were enabled and printed above, review them for clues on column counts per row.\")\n",
        "    elif insert_statements_found > 0 and rows_processed_count == 0 :\n",
        "        print(\"\\nWarning: Found INSERT statements, but could not identify any processable data rows.\")\n",
        "    elif insert_statements_found == 0:\n",
        "        print(\"\\nWarning: No INSERT statements were found in the SQL file.\")\n",
        "    else:\n",
        "        print(f\"\\nCSV file '{csv_file_path}' might be empty or not created due to issues.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: SQL input file not found at '{sql_file_path}'. Please ensure the path is correct.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWnoODRwSwoT",
        "outputId": "affe1205-0d81-4ac7-da0e-52d5fd0d31a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/244.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    }
  ]
}